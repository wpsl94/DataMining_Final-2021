sel.feature <- names(sel$selection)
acc <- evaluator(sel.feature)
acc
### 3. raw data + log변환
ds <- HV.log.norm[,-101]
cl <- HV.log.norm[,101]
sel <- MRMR(ds, cl, 10) # 3: #of selected feature
sel
sel.feature <- names(sel$selection)
acc <- evaluator(sel.feature)
acc
### 4. raw data
ds <- HV.scl[,-101]
cl <- HV.scl[,101]
sel <- MRMR(ds, cl, 10) # 3: #of selected feature
sel
sel.feature <- names(sel$selection)
acc <- evaluator(sel.feature)
acc
library(forecast)
#4. hill climbing search
HV.hcs <- HV[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
dim(HV.hcs)
#5. best first search
HV.bfs <- HV[,c(3,6,9,16,90,101)]
dim(HV.bfs)
#6. backward search
HV.bs <- HV[, c(1:74, 76:101)]
dim(HV.bs)
#7.forward search
HV.fs <- HV[, c(3,6,9,16,90,101)]
dim(HV.fs)
lm1 <- lm(class~., data=HV)
#result model
#1. raw dataset
HV
lm1 <- lm(class~., data=HV)
#2. preprocessed removed dataset
log.HV.rm.norm
lm2 <- lm(class~., data=log.HV.rm.norm)
#3. Custom HV test model
HV.model
lm3 <- lm(class~., data=HV.model)
#4. hill climbing search
HV.hcs <- HV[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
dim(HV.hcs)
lm4 <- lm(class~., data=HV.hcs)
#5. best first search
HV.bfs <- HV[,c(3,6,9,16,90,101)]
dim(HV.bfs)
lm5 <- lm(class~., data=HV.bfs)
#6. backward search
HV.bs <- HV[, c(1:74, 76:101)]
dim(HV.bs)
lm6 <- lm(class~., data=HV.bs)
#7.forward search
HV.fs <- HV[, c(3,6,9,16,90,101)]
dim(HV.fs)
lm7 <- lm(class~., data=HV.fs)
HV.model
lm3 <- lm(class~., data=HV.model)
View(lm5)
View(lm4)
View(lm7)
View(lm6)
accuracy(lm1)
accuracy(lm1)
accuracy(lm2)
accuracy(lm3)
accuracy(lm4)
accuracy(lm5)
accuracy(lm6)
accuracy(lm7)
class(HV.model$class)
lm3 <- lm(class~., data=HV.model.raw)
accuracy(lm3)
plot(log.HV.rm.norm)
plot(log.HV.rm.norm$class)
plot(log.HV.rm.norm$class)
subset1
class(subset1)
result1 <- Kfold.eval(ds,cl,sel.feature)
result2 <- Kfold.eval(ds,cl,sel.feature)
result3 <- Kfold.eval(ds,cl,sel.feature)
##raw
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result1 <- Kfold.eval(ds,cl,sel.feature)
##pre-processed(oulier removed + log transformed + normalization)
log.HV.rm.norm.ds <- log.HV.rm.norm
log.HV.rm.norm.ds$class <- as.factor(log.HV.rm.norm.ds$class)
ds <- log.HV.rm.norm.ds[,-101]
cl <- log.HV.rm.norm.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result2 <- Kfold.eval(ds,cl,sel.feature)
##made
ds <- HV.model[,-1]
cl <- HV.model[,1]
sel <- MRMR(ds, cl, 51) # selected feature
sel
sel.feature <- names(sel$selection)
result3 <- Kfold.eval(ds,cl,sel.feature)
data <- HV
glimpse(data)# 데이터 확인. dplyr 패키지에 내장
#random forest를 사용하여 예측.
#cross validation, using rf to predict data
k = 10
data$id <- sample(1:k, nrow(data), replace = TRUE)
list <- 1:k
# 예측 및 테스트는 폴드를 반복 할 때마다 추가되는 데이터 프레임을 설정합니다.
# prediction and test set data frames that we add to with each iteration over the folds.
# 데이터 프레임 초기화(data frame reset)
prediction <- testsetCopy <- data.frame()
# 코드 실행 시 작업진행률을 보여주는 progress.bar
#Creating a progress bar to know the status of CV
progress.bar <- create_progress_bar("text") # plyr 패키지안에 내장
progress.bar$init(k)
#function for k fold
#i는 1부터 10으로 나눈후에 10번을 진행하도록 합니다.
for(i in 1:k){
# remove rows with id i from dataframe to create training set
# ex) id가 1인 것을 제외하고 나머지 id 2~10를 training set으로 사용
# select rows with id i to create test set
# ex) id가 1인 것만 test set으로 사용
trainset <- subset(data, id %in% list[-i])
testset <- subset(data, id %in% c(i))
# 랜덤포레스트 모델을 생성.
#run a random forest model
model <- randomForest(trainset$class ~ .-id, data = trainset, ntree = 100)
temp <- as.data.frame(predict(model, testset))
# 예측값을 예측 데이터 프레임의 끝에 추가.
# append this iteration's predictions to the end of the prediction data frame
prediction <- rbind(prediction, temp)
# 실제값(testset) testsetCopy에 추가.
# append this iteration's test set to the test set copy data frame
testsetCopy <- rbind(testsetCopy, as.data.frame(testset[,1]))
progress.bar$step()
}
# 예측값과 실제값 데이터프레임.
# add predictions and actual Sepal Length values
result <- cbind(prediction, testsetCopy[, 1])
names(result) <- c("Predicted", "Actual")
result$Difference <- abs(result$Actual - result$Predicted)
# 모델 평가로 MAE[Mean Absolute Error] 사용.
# As an example use Mean Absolute Error as Evalution
summary(result$Difference)
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)
summary(lm6)
summary(lm7)
##중복데이터 제거
ds <- HV[,-101]
dup = which(duplicated(ds))
dup
#중복데이터 없음
HV.skew <- c(1:100)
for(i in 1:100){
HV.skew[i] <- skewness(HV[,i])
cat("X", i, "skewness: ", HV.skew[i], "\n")
}
min(HV.skew)
max(HV.skew)
plot(HV.skew[14])
plot(HV.skew)
plot(HV$X14)
hist(HV$X14)
hist(HV$X89)
par(1,2)
hist(HV$X14)
hist(HV$X89)
par(1,2){
hist(HV$X14)
hist(HV$X89)
}
par(mfrow=c(1,2))
hist(HV$X14)
hist(HV$X89)
par(mfrow=c(1,2))
hist(HV$X89)
hist(HV$X14)
HV.scl
par(mfrow=c(1,2))
hist(HV.scl$X89)
hist(HV.scl$X14)
hist(HV.scl$X89)
hist(HV.scl$X14)
HV.scl <- as.data.frame(scale(HV), class = HV[, 101])
hist(HV.scl$X89)
hist(HV.scl$X14)
HV.log.scl <- as.data.frame(scale(HV.log),  class = HV[,101])
par(mfrow=c(1,2))
hist(HV.scl$X89)
hist(HV.scl$X14)
hist(HV.log.scl$X89)
hist(HV.log.scl$X14)
idx
dim(HV.rm)
source('~/final project/00_library.R')
end-start
result.subset1 <- evaluator(subset1) # accuracy of subset
subset1
subset3
##raw
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result1 <- Kfold.eval(ds,cl,sel.feature)
##pre-processed(oulier removed + log transformed + normalization)
log.HV.rm.norm.ds <- log.HV.rm.norm
log.HV.rm.norm.ds$class <- as.factor(log.HV.rm.norm.ds$class)
ds <- log.HV.rm.norm.ds[,-101]
cl <- log.HV.rm.norm.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result2 <- Kfold.eval(ds,cl,sel.feature)
##made
ds <- HV.model[,-1]
cl <- HV.model[,1]
sel <- MRMR(ds, cl, 51) # selected feature
sel
sel.feature <- names(sel$selection)
result3 <- Kfold.eval(ds,cl,sel.feature)
##
result1 <- evaluator(sel.feature)
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result1 <- evaluator(sel.feature)
##raw
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result1 <- evaluator(sel.feature)
##pre-processed(oulier removed + log transformed + normalization)
log.HV.rm.norm.ds <- log.HV.rm.norm
log.HV.rm.norm.ds$class <- as.factor(log.HV.rm.norm.ds$class)
ds <- log.HV.rm.norm.ds[,-101]
cl <- log.HV.rm.norm.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature <- names(sel$selection)
result2 <- evaluator(sel.feature)
##made
ds <- HV.model[,-1]
cl <- HV.model[,1]
sel <- MRMR(ds, cl, 51) # selected feature
sel
sel.feature <- names(sel$selection)
result3 <- evaluator(sel.feature)
##raw
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature1 <- names(sel$selection)
result1 <- evaluator(sel.feature1)
##pre-processed(oulier removed + log transformed + normalization)
log.HV.rm.norm.ds <- log.HV.rm.norm
log.HV.rm.norm.ds$class <- as.factor(log.HV.rm.norm.ds$class)
ds <- log.HV.rm.norm.ds[,-101]
cl <- log.HV.rm.norm.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature2 <- names(sel$selection)
result2 <- evaluator(sel.feature2)
##made
ds <- HV.model[,-1]
cl <- HV.model[,1]
sel <- MRMR(ds, cl, 51) # selected feature
sel
sel.feature3 <- names(sel$selection)
result3 <- evaluator(sel.feature3)
sel.feature2
dim(HV.model)
str(HV.model)
summary(mod.select)
newdata <- HV.model
mod <- lm(class~., data = newdata)
mod.select <- stepAIC(mod)
mod.select
summary(mod.select)
newdata <- HV.log.norm
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.log.norm[, 1:100]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.log.norm$class
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- HV.model
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.model[, -1]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.model$class
#test
pred == answer
acc <- mean(pred==answer)
acc
plot(HV.model$X1,dnorm(HV.model$X1, mean=0, sd=1))
newdata <- HV.model
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.model[, -1]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.model$class
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- HV.model
mod.graw <- glm(class ~ ., data = newdata)
mod.graw <- glm(class ~ ., data = newdata)
HV.model$class <- as.factor(HV.model$class)
newdata <- HV.model
mod.graw <- glm(class ~ ., data = newdata)
################################################################################################
#FS final model
HV.model <- mod.select$model
newdata <- HV.model
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.model[, -1]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.model$class
#test
pred == answer
acc <- mean(pred==answer)
acc
plot(HV.model$X1,dnorm(HV.model$X1, mean=0, sd=1))
newdata <- HV.log.norm
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.log.norm[, 1:100]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.log.norm$class
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- HV.log
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
newdata <- HV.rm
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- HV.rm[,1:100]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- HV.rm$class
pred == answer
acc <- mean(pred==answer)
acc
newdata <- log.HV.rm.norm
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- log.HV.rm.norm[,1:100]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- log.HV.rm.norm$class
#test
pred == answer
acc <- mean(pred==answer)
acc
#multi linear
summary(lm1)
summary(lm2)
summary(lm3)
summary(lm4)
summary(lm5)
summary(lm6)
summary(lm7)
sult2 <- evaluator(sel.feature2)
e
result2 <- evaluator(sel.feature2)
#log transform
HV.temp <- HV[,-101]
HV.log <- log(HV.temp)
HV.log
HV.log <- data.frame(HV.log, class = HV[,101])
##데이터셋의 범위의 편차가 너무 크고 편향되어 있어
##Scaling 및 Normalization이 필요함
HV.log
evaluator(HV.fs)
result3
sel.feature3
sel
lm(class~., HV.model)
summary(lm(class~., HV.model))
mean(result.subset1, result.subset2, result.subset3, result.subset4)
summary(lm3)
result1 <- evaluator(sel.feature1)
##raw
HV.ds <- HV
HV.ds$class <- as.factor(HV$class)
str(HV.ds$class)
ds <- HV.ds[,-101]
cl <- HV.ds[,101]
sel <- MRMR(ds, cl, 100) # selected feature
sel
sel.feature1 <- names(sel$selection)
result1 <- evaluator(sel.feature1)
result1
result2
result3
rownames(HV.model)
colnames(HV.model)
dim(HV.model)
newdata <- log.HV.rm.norm[,c(3,6,9,16,90,101)]
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- log.HV.rm.norm[,c(3,6,9,16,90,101)]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- log.HV.rm.norm[,101]
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- log.HV.rm.norm[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- log.HV.rm.norm[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- log.HV.rm.norm[,101]
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- log.HV.rm.norm[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- log.HV.rm.norm[,c(1, 2, 3, 7,9,10,12,13,14,17,18,19,20,21,23,24,25,26,27,28,29,30,35,36,38,40,41,42,43,44,46,47,49,50,52,54,57,58,62,64,65,71,73,81,82,83,84,86,87,88,91,94,96,97,98,99, 101)]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- log.HV.rm.norm[,101]
#test
pred == answer
acc <- mean(pred==answer)
acc
newdata <- log.HV.rm.norm[,c(1:74, 76:101)]
mod.graw <- glm(class ~ ., data = newdata)
mod.graw
summary(mod.graw)
test <- log.HV.rm.norm[,c(1:74, 76:101)]
pred <- predict(mod.graw, test)
pred <- round(pred, 0)
pred
answer <- log.HV.rm.norm[,101]
#test
pred == answer
acc <- mean(pred==answer)
acc
